#-----------------------------------------
#
# SqlLex.py - a lexer for small database
#
#-----------------------------------------
import ply.lex as lex


# List of token names. This is always required
reserved = {
    'define' : 'DEFINE',
    'relation'  : 'RELATION',
    'ATTRIBUTE' : 'attribute',
    'SET'       : 'set',
    'PRIMARY'   : 'primary',
    'KEY'       : 'key',
    'CREATE'    : 'create',
    'TABLE'     : 'table',
    'INSERT'    : 'insert',
    'DELETE'    : 'delete',
    'UPDATE'    : 'update',
    'SELECT'    : 'select',
    'FROM'      : 'from',
    'WHERE'     : 'where',
    'INTEGER'   : 'integer',
    'CHARACTER' : 'character',
    'RANGE'     : 'range'
}

tokens = list(reserved.values()) + [
    'EQUAL',
    'GREATER',
    'LESS',
    'WORD',
    'NUMBER'
]

# Regular expression rules for simple tokens
t_EQUAL     = r'='
t_GREATER   = r'>'
t_LESS      = r'<'
t_WORD      = r'\w+'

# A regular expression rule with some action code
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)
    return t

# Define a rule so we can track line numbers
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)

# A string containing ignored characters(spaces and tabs)
t_ignore = ' \t'

# Error handling rule
def t_error(t):
    print("Illegal character" + t.value[0])
    t.lexer.skip(1)

# Build the lexer
lexer = lex.lex()



# Test it out 
#data = 'select SPOINT2 from STUDENT_DB where SPOINT1<70'

# Give the lexer some input
#lexer.input(data)

#Tokenize
#while True:
#    tok = lexer.token()
#    if not tok: break  # No more input
#    print tok
